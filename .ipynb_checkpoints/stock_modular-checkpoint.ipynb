{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import Series\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import LSTM, TimeDistributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose CSV file of the company stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL.csv\t\t      NFLX.csv\r\n",
      "AMZN.csv\t\t      __pycache__\r\n",
      "assessment_dsml1_train.csv    stock_LSTM-Copy1.ipynb\r\n",
      "click_pred.ipynb\t      stock_LSTM.ipynb\r\n",
      "DSML1 - 4 - Assessment.ipynb  stock_LSTM-modular.ipynb\r\n",
      "FB.csv\t\t\t      Stock_LSTM_n.ipynb\r\n",
      "FBdata.csv\t\t      stock_modular.ipynb\r\n",
      "fb.ipynb\t\t      Stock-Predictor-using-LSTM.ipynb\r\n",
      "files\t\t\t      Stock_prices_m.zip\r\n",
      "GOOG.csv\t\t      Test.ipynb\r\n",
      "lstm.py\t\t\t      TSLA.csv\r\n",
      "model.h5\t\t      TSLAdata.csv\r\n",
      "model.json\t\t      Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mapr_data/auro_157072/business_news/news_02/Stock/Files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b1485e3af543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34mr'/mapr_data/auro_157072/business_news/news_02/Stock/Files'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mapr_data/auro_157072/business_news/news_02/Stock/Files'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path =r'/mapr_data/auro_157072/business_news/news_02/Stock/Files'\n",
    "\n",
    "os.chdir(path)\n",
    "arr = os.listdir()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path =r'/mapr_data/auro_157072/business_news/news_02/Stock/Files'\n",
    "filenames = glob.glob(path + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(path +\"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for index,filename in enumerate(filenames):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    close_name = 'Close_' + arr[index][:-4]\n",
    "    col_names = ['Timestamp','Sequence','Open','High','Low',close_name,'Volume']\n",
    "    df.columns = col_names\n",
    "    df['Timestamp'] =  pd.to_datetime(df['Timestamp'],format='%Y-%m-%d %H:%M:%S')\n",
    "    df.index = df['Timestamp']\n",
    "    df = df.drop(['Sequence','Timestamp'],axis =1)\n",
    "    \n",
    "    df = df[[close_name]]\n",
    "    df = df.loc['2018-03-14']\n",
    "    \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(dfs, axis=1)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[:, [2]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    #plt.subplot(i,1,1)\n",
    "    result.iloc[:, [i]].plot()\n",
    "    plt.title(arr[i][:-4])\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =result.iloc[:-50]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = result.iloc[-50:]\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[:, [2]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains,train_scaleds,tests,test_scaleds  = [],[],[],[]\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    series = result.iloc[:, [i]]\n",
    "    raw_values = series.values\n",
    "    diff_values = difference(raw_values, 1)\n",
    "    supervised = timeseries_to_supervised(diff_values, 4)\n",
    "    supervised_values = supervised.values\n",
    "    train, test = supervised_values[0:-100], supervised_values[-100:]\n",
    "    \n",
    "    trains.append(train)\n",
    "    tests.append(test)\n",
    "    \n",
    "    scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    train_scaleds.append(train_scaled)\n",
    "    test_scaleds.append(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train = np.insert(trains[0][:,:-1],[4],trains[1][:,:-1], axis=1)\n",
    "merge_test = np.insert(tests[0][:,:-1],[4],tests[1][:,:-1], axis=1)\n",
    "\n",
    "for i in range(len(dfs)-2):\n",
    "    merge_train = np.insert(merge_train,[4 * (i+2)],trains[i+2][:,:-1], axis=1)\n",
    "    merge_test  = np.insert(merge_test,[4 * (i+2)],tests[i+2][:,:-1], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_train.shape)\n",
    "print(merge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_trains = []\n",
    "l_tests = []\n",
    "for i in range(len(dfs)):\n",
    "\n",
    "    l_train = trains[i][:,-1].reshape(trains[i][:,-1].shape[0],1)\n",
    "    l_test = tests[i][:,-1].reshape(tests[i][:,-1].shape[0],1)\n",
    "    l_trains.append(l_train)\n",
    "    l_tests.append(l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_train_output = np.concatenate(tuple(l_trains),axis=1)\n",
    "merge_test_output = np.concatenate(tuple(l_tests),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_train_output.shape)\n",
    "print(merge_test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import livelossplot\n",
    "plot_losses = livelossplot.PlotLossesKeras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = merge_train, merge_train_output\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "y = y.reshape(y.shape[0], 1, y.shape[1])\n",
    "\n",
    "X_test, y_test = merge_test, merge_test_output\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "y_test = y_test.reshape(y_test.shape[0], 1, y_test.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "model.add(LSTM(12, batch_input_shape=(1, X.shape[1], X.shape[2]), stateful=False,return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(18,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(8,return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=3))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "'''\n",
    "\n",
    "dim_in = 12\n",
    "dim_out = len(dfs)\n",
    "nb_units = 8\n",
    "\n",
    "\n",
    "model.add(LSTM(input_shape=(1, dim_in),\n",
    "                    return_sequences=True, \n",
    "                    units=nb_units))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Dense(activation='linear', units=dim_out)))\n",
    "model.compile(loss = 'mse', optimizer = 'rmsprop')\n",
    "\n",
    "#print(X.shape)\n",
    "history = model.fit(X, y, epochs = 200, batch_size = 1,verbose=0,validation_data=(X_test, y_test),callbacks=[plot_losses])\n",
    "              \n",
    "              \n",
    "#model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "#model.add(Dense(1))\n",
    "#model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "#model.fit(X, y, epochs=nb_epoch, batch_size=batch_size, verbose=1, shuffle=False,callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min')])\n",
    "\n",
    "#for i in range(500):\n",
    "#print(\"Completed :\",i+1,\"/\",500, \"Steps\")\n",
    "#model.fit(X, y, epochs=500, batch_size=1, verbose=0, shuffle=False,callbacks=[plot_losses])\n",
    "#model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_lstm(model, X):\n",
    "    y = model.predict(X)\n",
    "    return y\n",
    "\n",
    "ts = []\n",
    "base_values = []\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    t = merge_train[-1:,4*i : 4*(i+1)]\n",
    "    ts.append(t)\n",
    "    base_values.append(result.iloc[290][i])\n",
    "\n",
    "predictions = []\n",
    "\n",
    "pred_apple = []\n",
    "pred_amazon = []\n",
    "pred_facebook = []\n",
    "\n",
    "prediction = np.zeros((100,3))\n",
    "prediction_i = np.zeros((100,3))\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    temp = []\n",
    "    for j in range(len(dfs)):\n",
    "        temp.append(ts[j][:,i:])\n",
    "    \n",
    "    test_pred = np.concatenate(tuple(temp),axis=1)\n",
    "    input_data = test_pred.reshape(1, 1, 4 *  len(dfs))\n",
    "    \n",
    "    y = forecast_lstm(model,input_data)\n",
    "    prediction[i] = y[0]\n",
    "    \n",
    "    ys =[]\n",
    "    \n",
    "    for k in range(len(dfs)):\n",
    "        ts[k] = np.insert(ts[k],[i+4],y[0][0][k], axis=1)\n",
    "        \n",
    "        temp = base_values[k] + y[0][0][k]\n",
    "        prediction_i[i][k] = temp\n",
    "    \n",
    "sqrt(mean_squared_error(merge_test_output, prediction))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for i in range(len(dfs)):\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.plot(prediction_i[:,i])\n",
    "    plt.plot(result.iloc[:, [i]].iloc[-100:].values)\n",
    "    plt.title(arr[i][:-4])\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallal Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_train.shape, merge_train_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(features,labels):\n",
    "    \n",
    "    dim_in = 4\n",
    "    dim_out = 1\n",
    "    nb_units = 6\n",
    "    \n",
    "    features = features.reshape(features.shape[0], 1, features.shape[1])   \n",
    "    labels = labels.reshape(labels.shape[0], 1, labels.shape[1])   \n",
    "    \n",
    "    feature1, label1 = merge_train[:,0:4],merge_train_output[:,0:1]\n",
    "    feature1 = feature1.reshape(feature1.shape[0], 1, feature1.shape[1])   \n",
    "    label1 = label1.reshape(label1.shape[0], 1, label1.shape[1])\n",
    "    \n",
    "    feature2, label2 = merge_train[:,4:8],merge_train_output[:,1:2]\n",
    "    feature2 = feature2.reshape(feature2.shape[0], 1, feature2.shape[1])   \n",
    "    label2 = label2.reshape(label2.shape[0], 1, label2.shape[1])\n",
    "    \n",
    "    feature3, label3 = merge_train[:,8:12],merge_train_output[:,2:3]\n",
    "    feature3 = feature3.reshape(feature3.shape[0], 1, feature3.shape[1])   \n",
    "    label3 = label3.reshape(label3.shape[0], 1, label3.shape[1])\n",
    "    \n",
    "    \n",
    "    #joining the models\n",
    "    inp1 = Input((1,4))\n",
    "\n",
    "    #two inputs for model 2 (the model we want to run twice as fast)\n",
    "    inp2 = Input((1,4))\n",
    "    inp3 = Input((1,4))\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(batch_input_shape=(1, features.shape[1], features.shape[2]),\n",
    "                    return_sequences=True, \n",
    "                    units=8))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model.add(LSTM(6,return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(4,return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(activation='linear', units=dim_out)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    out1 = model(inp1)\n",
    "    out2 = model(inp2)\n",
    "    out3 = model(inp3)\n",
    "    \n",
    "    \n",
    "    #out1 = model(feature1) #use model 2 once\n",
    "    #out2 = model(feature2) \n",
    "    \n",
    "    models = Model([inp1,inp2,inp3],[out1,out2,out3])\n",
    "    \n",
    "    #model.compile(loss = 'mse', optimizer = 'rmsprop')\n",
    "    models.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    #model.fit(X, y, epochs=nb_epoch, batch_size=batch_size, verbose=1, shuffle=False,callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min')])\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(100):\n",
    "        print(\"Completed :\",i+1,\"/\",500, \"Steps\")\n",
    "        #model.fit(features, labels, epochs=1, batch_size=1, verbose=0, shuffle=False)\n",
    "        models.fit([feature1,feature2,feature3],[label1,label2,label3], epochs = 1,verbose=0,shuffle=False)\n",
    "        models.reset_states()\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_train[:,0:4].shape)\n",
    "print(merge_train_output[:,0:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = fit_lstm(merge_train[:,0:4],merge_train_output[:,0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.copy(merge_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = np.zeros((100,3))\n",
    "prediction_i = np.zeros((100,3))\n",
    "input_data = np.copy(merge_train[-1,:])\n",
    "\n",
    "\n",
    "base_values = []\n",
    "\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    base_values.append(result.iloc[290][i])\n",
    "\n",
    "for i in range(100):\n",
    "    ts = []\n",
    "    \n",
    "    for m in range(len(dfs)):\n",
    "        t = input_data[4*m + i : 4*(m+1) + i]\n",
    "        t = t.reshape(1,1,4)\n",
    "        \n",
    "        ts.append(t)\n",
    "    \n",
    "    y = models.predict(ts)\n",
    "    prediction[i] = np.concatenate(y, axis=1 ).reshape((1,1,len(dfs)))    \n",
    "    for k in range(len(dfs)):\n",
    "        input_data = np.insert(input_data,[i+4*(k+1)],y[k][0][0], axis=0)\n",
    "        prediction_i[i][k] = base_values[k] + y[k][0][0]    \n",
    "sqrt(mean_squared_error(merge_test_output, prediction))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for i in range(len(dfs)):\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.plot(prediction_i[:,i])\n",
    "    plt.title(arr[i][:-4])\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
